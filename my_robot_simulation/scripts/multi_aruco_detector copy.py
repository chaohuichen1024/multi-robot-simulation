#!/usr/bin/env python3
import rospy
import cv2
import cv2.aruco as aruco
import numpy as np
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge, CvBridgeError
import ast
import tf
import tf2_ros
import geometry_msgs.msg
import math
from typing import List, Tuple
from std_msgs.msg import Header
from geometry_msgs.msg import Point, Pose, PoseArray, Quaternion

class PerspectiveCorrector:
    def __init__(self):
        # è¾¹ç¼˜æ£€æµ‹å‚æ•°
        self.canny_threshold = 50
        self.canny_threshold_max = 200
        self.hough_threshold = 50
        self.min_line_length = 100
        self.max_line_gap = 10
        
        # æœ€å¤§æ£€æµ‹ç›´çº¿æ•°é‡
        self.max_lines_num = 12
        
    def auto_perspective_correction(self, image: np.ndarray) -> np.ndarray:
        """
        è‡ªåŠ¨é€è§†å˜æ¢çŸ«æ­£ä¸»å‡½æ•°
        """
        if image is None:
            raise ValueError("è¾“å…¥å›¾åƒä¸ºç©º")
            
        # 1. é¢„å¤„ç†å›¾åƒ
        gray = self.preprocess_image(image)
        
        # 2. è¾¹ç¼˜æ£€æµ‹å’Œç›´çº¿æå–
        lines = self.edge_detection_and_line_extraction(gray)
        
        if len(lines) < 4:
            rospy.logdebug("æ£€æµ‹åˆ°çš„ç›´çº¿æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œé€è§†å˜æ¢")
            return image
        
        # 3. æå–å’Œè¿‡æ»¤å…³é”®ç‚¹
        points = self.extract_and_filter_points(lines, image.shape)
        
        if len(points) < 4:
            rospy.logdebug("æå–çš„å…³é”®ç‚¹æ•°é‡ä¸è¶³")
            return image
        
        # 4. è®¡ç®—å››ä¸ªè§’ç‚¹
        corners = self.calculate_four_corners(points, image.shape)
        
        if corners is None:
            rospy.logdebug("æ— æ³•è®¡ç®—å››ä¸ªè§’ç‚¹")
            return image
        
        # 5. æ‰§è¡Œé€è§†å˜æ¢
        result = self.apply_perspective_transform(image, corners)
        
        return result
    
    def preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """
        å›¾åƒé¢„å¤„ç†
        """
        # è½¬æ¢ä¸ºç°åº¦å›¾
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image.copy()
        
        # é«˜æ–¯æ¨¡ç³Šå»å™ª
        gray = cv2.GaussianBlur(gray, (5, 5), 0)
        
        return gray
    
    def edge_detection_and_line_extraction(self, gray: np.ndarray) -> List:
        """
        è¾¹ç¼˜æ£€æµ‹å’Œç›´çº¿æå–
        """
        lines = []
        canny_threshold = self.canny_threshold
        
        # è¿­ä»£è°ƒæ•´Cannyé˜ˆå€¼ï¼Œç›´åˆ°æ£€æµ‹åˆ°çš„ç›´çº¿æ•°é‡åˆé€‚
        while canny_threshold <= self.canny_threshold_max:
            # Cannyè¾¹ç¼˜æ£€æµ‹
            edges = cv2.Canny(gray, canny_threshold, canny_threshold * 2)
            
            # éœå¤«ç›´çº¿æ£€æµ‹
            detected_lines = cv2.HoughLinesP(
                edges, 1, np.pi/180, self.hough_threshold,
                minLineLength=self.min_line_length, 
                maxLineGap=self.max_line_gap
            )
            
            if detected_lines is None:
                canny_threshold += 10
                continue
                
            detected_lines = detected_lines.reshape(-1, 4).tolist()
            
            # è¿‡æ»¤è¿‡äºè´´è¿‘è¾¹ç¼˜çš„ç›´çº¿
            filtered_lines = self.filter_edge_lines(detected_lines, gray.shape)
            
            if len(filtered_lines) <= self.max_lines_num:
                lines = filtered_lines
                break
            else:
                canny_threshold += 10
        
        rospy.logdebug(f"æœ€ç»ˆæ£€æµ‹åˆ° {len(lines)} æ¡ç›´çº¿")
        return lines
    
    def filter_edge_lines(self, lines: List, image_shape: Tuple) -> List:
        """
        è¿‡æ»¤è¿‡äºè´´è¿‘å›¾åƒè¾¹ç¼˜çš„ç›´çº¿
        """
        height, width = image_shape
        margin = 20  # è¾¹ç¼˜é˜ˆå€¼
        
        filtered_lines = []
        
        for line in lines:
            x1, y1, x2, y2 = line
            
            # è¿‡æ»¤æ°´å¹³æˆ–å‚ç›´çš„çŸ­ç›´çº¿
            if (abs(x1 - x2) < 10 and abs(y1 - y2) > 10) or \
               (abs(y1 - y2) < 10 and abs(x1 - x2) > 10):
                continue
            
            # æ£€æŸ¥æ˜¯å¦è¿‡äºè´´è¿‘è¾¹ç¼˜
            if (x1 < margin and x2 < margin) or \
               (x1 > width - margin and x2 > width - margin) or \
               (y1 < margin and y2 < margin) or \
               (y1 > height - margin and y2 > height - margin):
                continue
                
            filtered_lines.append(line)
        
        return filtered_lines
    
    def extract_and_filter_points(self, lines: List, image_shape: Tuple) -> List[Tuple]:
        """
        ä»ç›´çº¿ä¸­æå–å…³é”®ç‚¹å¹¶è¿‡æ»¤
        """
        points = []
        
        # æå–æ‰€æœ‰çº¿æ®µçš„ç«¯ç‚¹
        for line in lines:
            x1, y1, x2, y2 = line
            points.append((x1, y1))
            points.append((x2, y2))
        
        # è¿‡æ»¤ç›¸è¿‘çš„ç‚¹
        filtered_points = self.filter_close_points(points)
        
        # æŒ‰åˆ°åŸç‚¹çš„è·ç¦»æ’åº
        filtered_points.sort(key=lambda p: p[0] + p[1])
        
        return filtered_points
    
    def filter_close_points(self, points: List[Tuple], threshold: int = 10) -> List[Tuple]:
        """
        è¿‡æ»¤è·ç¦»è¿‡è¿‘çš„ç‚¹
        """
        filtered_points = []
        
        for i, point1 in enumerate(points):
            keep_point = True
            
            for j, point2 in enumerate(points):
                if i == j:
                    continue
                    
                distance = math.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)
                
                if distance < threshold:
                    # å¦‚æœæ‰¾åˆ°æ›´æ¥è¿‘åŸç‚¹çš„ç‚¹ï¼Œä¿ç•™é‚£ä¸ªç‚¹
                    if (point2[0] + point2[1]) < (point1[0] + point1[1]):
                        keep_point = False
                        break
            
            if keep_point and point1 not in filtered_points:
                filtered_points.append(point1)
        
        return filtered_points
    
    def calculate_four_corners(self, points: List[Tuple], image_shape: Tuple) -> np.ndarray:
        """
        è®¡ç®—å››ä¸ªè§’ç‚¹
        """
        if len(points) < 4:
            return None
        
        height, width = image_shape[0], image_shape[1]
        
        # æ–¹æ³•1ï¼šåŸºäºè·ç¦»çš„æ–¹æ³•
        corners1 = self.method1_distance_based(points.copy())
        
        # æ–¹æ³•2ï¼šåŸºäºå€¾æ–œæ–¹å‘çš„æ–¹æ³•
        corners2 = self.method2_tilt_based(points.copy(), image_shape)
        
        # å¦‚æœä¸¤ç§æ–¹æ³•éƒ½æˆåŠŸï¼Œå–å¹³å‡å€¼
        if corners1 is not None and corners2 is not None:
            # æ£€æŸ¥ä¸¤ä¸ªç»“æœæ˜¯å¦æ¥è¿‘
            avg_corners = self.average_corners(corners1, corners2)
            return avg_corners
        elif corners1 is not None:
            return corners1
        elif corners2 is not None:
            return corners2
        else:
            return None
    
    def method1_distance_based(self, points: List[Tuple]) -> np.ndarray:
        """
        æ–¹æ³•1ï¼šåŸºäºæœ€å¤§è·ç¦»çš„è§’ç‚¹è®¡ç®—
        """
        if len(points) < 4:
            return None
        
        # å·¦ä¸Šå’Œå³ä¸‹ç‚¹ï¼ˆæ’åºåçš„ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªï¼‰
        left_top = points[0]
        right_down = points[-1]
        
        # åˆ†ç¦»å³ä¸Šå’Œå·¦ä¸‹ç‚¹ç°‡
        right_top_cluster = []
        left_down_cluster = []
        
        for point in points[1:-1]:  # æ’é™¤å·²ç»ç¡®å®šçš„ä¸¤ä¸ªç‚¹
            x, y = point
            
            # å³ä¸Šç‚¹ç°‡ï¼šx > å·¦ä¸Šx ä¸” y < å³ä¸‹y
            if x > left_top[0] and y < right_down[1]:
                right_top_cluster.append(point)
            
            # å·¦ä¸‹ç‚¹ç°‡ï¼šy > å·¦ä¸Šy ä¸” x < å³ä¸‹x
            if y > left_top[1] and x < right_down[0]:
                left_down_cluster.append(point)
        
        if not right_top_cluster or not left_down_cluster:
            return None
        
        # åœ¨ç‚¹ç°‡ä¸­å¯»æ‰¾è·ç¦»æœ€è¿œçš„ç‚¹å¯¹
        max_distance = 0
        best_right_top = right_top_cluster[0]
        best_left_down = left_down_cluster[0]
        
        for rt_point in right_top_cluster:
            for ld_point in left_down_cluster:
                distance = (rt_point[0] - ld_point[0])**2 + (rt_point[1] - ld_point[1])**2
                if distance > max_distance:
                    max_distance = distance
                    best_right_top = rt_point
                    best_left_down = ld_point
        
        corners = np.array([
            left_top,
            best_right_top,
            right_down,
            best_left_down
        ], dtype=np.float32)
        
        return corners
    
    def method2_tilt_based(self, points: List[Tuple], image_shape: Tuple) -> np.ndarray:
        """
        æ–¹æ³•2ï¼šåŸºäºå€¾æ–œæ–¹å‘çš„è§’ç‚¹è®¡ç®—
        """
        if len(points) < 4:
            return None
        
        height, width = image_shape[0], image_shape[1]
        
        left_top = points[0]
        right_down = points[-1]
        
        # åˆ†ç¦»ç‚¹ç°‡
        right_top_cluster = []
        left_down_cluster = []
        
        for point in points[1:-1]:
            x, y = point
            
            if x > left_top[0] and y < right_down[1]:
                right_top_cluster.append(point)
            
            if y > left_top[1] and x < right_down[0]:
                left_down_cluster.append(point)
        
        if not right_top_cluster or not left_down_cluster:
            return None
        
        # åˆ¤æ–­å›¾åƒå€¾æ–œæ–¹å‘
        image_state = self.determine_image_tilt(right_top_cluster, left_top)
        
        # æ ¹æ®å€¾æ–œæ–¹å‘ç¡®å®šçœŸæ­£çš„å³ä¸Šå’Œå·¦ä¸‹ç‚¹
        if image_state == "lean_to_right":
            # å‘å³å€¾æ–œï¼šå³ä¸Šç‚¹å–æ¨ªåæ ‡æœ€å¤§ï¼Œå·¦ä¸‹ç‚¹å–æ¨ªåæ ‡æœ€å°
            right_top_cluster.sort(key=lambda p: p[0], reverse=True)
            left_down_cluster.sort(key=lambda p: p[0])
        elif image_state == "lean_to_left":
            # å‘å·¦å€¾æ–œï¼šå³ä¸Šç‚¹å–çºµåæ ‡æœ€å°ï¼Œå·¦ä¸‹ç‚¹å–çºµåæ ‡æœ€å¤§
            right_top_cluster.sort(key=lambda p: p[1])
            left_down_cluster.sort(key=lambda p: p[1], reverse=True)
        else:
            # æ­£å¸¸çŠ¶æ€ï¼šä½¿ç”¨é»˜è®¤æ’åº
            right_top_cluster.sort(key=lambda p: p[0] + p[1])
            left_down_cluster.sort(key=lambda p: p[0] + p[1], reverse=True)
        
        true_right_top = right_top_cluster[0] if right_top_cluster else None
        true_left_down = left_down_cluster[0] if left_down_cluster else None
        
        if true_right_top is None or true_left_down is None:
            return None
        
        corners = np.array([
            left_top,
            true_right_top,
            right_down,
            true_left_down
        ], dtype=np.float32)
        
        return corners
    
    def determine_image_tilt(self, right_top_cluster: List[Tuple], left_top: Tuple) -> str:
        """
        åˆ¤æ–­å›¾åƒå€¾æ–œæ–¹å‘
        """
        # å¦‚æœæ‰€æœ‰å³ä¸Šç‚¹çš„yå€¼éƒ½å¤§äºå·¦ä¸Šç‚¹çš„yå€¼ï¼Œè¯´æ˜å›¾åƒå‘å³å€¾æ–œ
        all_y_greater = all(point[1] > left_top[1] for point in right_top_cluster)
        
        if all_y_greater:
            return "lean_to_right"
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾å‘å·¦å€¾æ–œçš„ç‰¹å¾
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„åˆ¤æ–­é€»è¾‘
            return "lean_to_left"
    
    def average_corners(self, corners1: np.ndarray, corners2: np.ndarray) -> np.ndarray:
        """
        å¹³å‡ä¸¤ç§æ–¹æ³•å¾—åˆ°çš„è§’ç‚¹
        """
        return (corners1 + corners2) / 2
    
    def apply_perspective_transform(self, image: np.ndarray, corners: np.ndarray) -> np.ndarray:
        """
        åº”ç”¨é€è§†å˜æ¢
        """
        # é‡æ–°æ’åˆ—è§’ç‚¹é¡ºåºï¼šå·¦ä¸Šã€å³ä¸Šã€å³ä¸‹ã€å·¦ä¸‹
        corners = self.reorder_corners(corners)
        
        # è®¡ç®—å˜æ¢åçš„å®½åº¦å’Œé«˜åº¦
        width = max(
            np.linalg.norm(corners[0] - corners[1]),
            np.linalg.norm(corners[2] - corners[3])
        )
        height = max(
            np.linalg.norm(corners[0] - corners[3]),
            np.linalg.norm(corners[1] - corners[2])
        )
        
        # ç›®æ ‡ç‚¹
        dst_points = np.array([
            [0, 0],
            [width, 0],
            [width, height],
            [0, height]
        ], dtype=np.float32)
        
        # è®¡ç®—é€è§†å˜æ¢çŸ©é˜µ
        matrix = cv2.getPerspectiveTransform(corners, dst_points)
        
        # åº”ç”¨é€è§†å˜æ¢
        result = cv2.warpPerspective(image, matrix, (int(width), int(height)))
        
        return result
    
    def reorder_corners(self, corners: np.ndarray) -> np.ndarray:
        """
        é‡æ–°æ’åˆ—è§’ç‚¹é¡ºåº
        """
        # è®¡ç®—ä¸­å¿ƒç‚¹
        center = np.mean(corners, axis=0)
        
        # æ’åºè§’ç‚¹
        def angle_from_center(point):
            return math.atan2(point[1] - center[1], point[0] - center[0])
        
        sorted_corners = sorted(corners, key=angle_from_center)
        
        # é‡æ–°æ’åˆ—ä¸ºï¼šå·¦ä¸Šã€å³ä¸Šã€å³ä¸‹ã€å·¦ä¸‹
        # æ‰¾åˆ°æœ€å·¦ä¸Šçš„ç‚¹ä½œä¸ºèµ·ç‚¹
        start_index = np.argmin([p[0] + p[1] for p in sorted_corners])
        
        reordered = []
        for i in range(4):
            reordered.append(sorted_corners[(start_index + i) % 4])
        
        return np.array(reordered, dtype=np.float32)

class EnhancedArucoDetector:
    def __init__(self):
        rospy.init_node('enhanced_aruco_detector', anonymous=True)
        
        self.bridge = CvBridge()
        
        # å®‰å…¨åœ°è·å–æœºå™¨äººåˆ—è¡¨å‚æ•°
        robot_names_param = rospy.get_param('~robot_names', '["robot1", "robot2", "robot3"]')
        
        # è§£ææœºå™¨äººåç§°åˆ—è¡¨
        try:
            if isinstance(robot_names_param, str):
                self.robot_names = ast.literal_eval(robot_names_param)
            else:
                self.robot_names = robot_names_param
        except:
            rospy.logwarn("æ— æ³•è§£ærobot_nameså‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            self.robot_names = ["robot1", "robot2", "robot3"]
        
        # ARç æ£€æµ‹å‚æ•°
        self.aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)
        self.parameters = aruco.DetectorParameters_create()
        
        # ARç å®é™…å°ºå¯¸ (0.2x0.2ç±³)
        self.marker_length = 0.2
        
        # å­˜å‚¨æ¯ä¸ªæœºå™¨äººçš„å‘å¸ƒå™¨å’Œç›¸æœºå‚æ•°
        self.image_publishers = {}
        self.camera_matrix = {}
        self.dist_coeffs = {}
        self.has_camera_info = {}
        self.undistort_maps = {}  # æ–°å¢ï¼šå­˜å‚¨ç•¸å˜çŸ«æ­£æ˜ å°„
        
        # æ–°å¢ï¼šARç åæ ‡å‘å¸ƒå™¨
        self.aruco_pose_publishers = {}
        
        # é€è§†çŸ«æ­£å™¨
        self.perspective_corrector = PerspectiveCorrector()
        
        # TFå¹¿æ’­å™¨
        self.tf_broadcaster = tf.TransformBroadcaster()
        
        # è®¾ç½®è®¢é˜…å™¨å’Œå‘å¸ƒå™¨
        self.setup_robot_connections()
        
        rospy.loginfo(f"å¢å¼ºç‰ˆARç æ£€æµ‹å™¨å·²å¯åŠ¨ï¼Œç›‘æ§æœºå™¨äºº: {self.robot_names}")
        
    def setup_robot_connections(self):
        """ä¸ºæ¯ä¸ªæœºå™¨äººè®¾ç½®è®¢é˜…å™¨å’Œå‘å¸ƒå™¨ï¼ŒåŒ…æ‹¬å‰æ–¹å’Œå³ä¾§ç›¸æœº"""
        for robot in self.robot_names:
            # åˆå§‹åŒ–ç›¸æœºå‚æ•°çŠ¶æ€
            self.has_camera_info[robot] = {
                'front': False,
                'right': False
            }
            self.camera_matrix[robot] = {
                'front': None,
                'right': None
            }
            self.dist_coeffs[robot] = {
                'front': None,
                'right': None
            }
            self.undistort_maps[robot] = {
                'front': None,
                'right': None
            }
            
            # æ„å»ºåˆæ³•çš„è¯é¢˜åç§°
            # å‰æ–¹ç›¸æœº
            front_image_topic = f"/{robot}/camera/image"
            front_camera_info_topic = f"/{robot}/camera/camera_info"
            front_result_topic = f"/aruco_detection/{robot}/front_result"
            
            # å³ä¾§ç›¸æœº
            right_image_topic = f"/{robot}/right_camera/image_right"
            right_camera_info_topic = f"/{robot}/right_camera/camera_info_right"
            right_result_topic = f"/aruco_detection/{robot}/right_result"
            
            # è®¢é˜…å‰æ–¹ç›¸æœºè¯é¢˜
            rospy.Subscriber(front_image_topic, Image, 
                           lambda msg, robot_name=robot, camera_type='front': self.image_callback(msg, robot_name, camera_type))
            rospy.Subscriber(front_camera_info_topic, CameraInfo,
                           lambda msg, robot_name=robot, camera_type='front': self.camera_info_callback(msg, robot_name, camera_type))
            
            # è®¢é˜…å³ä¾§ç›¸æœºè¯é¢˜
            rospy.Subscriber(right_image_topic, Image, 
                           lambda msg, robot_name=robot, camera_type='right': self.image_callback(msg, robot_name, camera_type))
            rospy.Subscriber(right_camera_info_topic, CameraInfo,
                           lambda msg, robot_name=robot, camera_type='right': self.camera_info_callback(msg, robot_name, camera_type))
            
            # åˆ›å»ºç»“æœå›¾åƒå‘å¸ƒå™¨
            self.image_publishers[robot] = {
                'front': rospy.Publisher(front_result_topic, Image, queue_size=1),
                'right': rospy.Publisher(right_result_topic, Image, queue_size=1)
            }
            
            # æ–°å¢ï¼šåˆ›å»ºARç åæ ‡å‘å¸ƒå™¨
            self.aruco_pose_publishers[robot] = {
                'front': rospy.Publisher(f"/aruco_detection/{robot}/front_poses", PoseArray, queue_size=1),
                'right': rospy.Publisher(f"/aruco_detection/{robot}/right_poses", PoseArray, queue_size=1)
            }
            
            rospy.loginfo(f"å·²è®¢é˜… {robot} å‰æ–¹ç›¸æœº: {front_image_topic}")
            rospy.loginfo(f"å·²è®¢é˜… {robot} å³ä¾§ç›¸æœº: {right_image_topic}")
    
    def camera_info_callback(self, msg, robot_name, camera_type):
        """å¤„ç†ç›¸æœºå‚æ•°ä¿¡æ¯å¹¶ç”Ÿæˆç•¸å˜çŸ«æ­£æ˜ å°„"""
        if not self.has_camera_info[robot_name][camera_type]:
            # æå–ç›¸æœºå†…å‚çŸ©é˜µ
            self.camera_matrix[robot_name][camera_type] = np.array(msg.K).reshape(3, 3)
            
            # æå–ç•¸å˜ç³»æ•°
            self.dist_coeffs[robot_name][camera_type] = np.array(msg.D)
            
            # ç”Ÿæˆç•¸å˜çŸ«æ­£æ˜ å°„ï¼ˆå®æ—¶çŸ«æ­£ç”¨ï¼‰
            image_size = (msg.width, msg.height)
            new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(
                self.camera_matrix[robot_name][camera_type],
                self.dist_coeffs[robot_name][camera_type],
                image_size, 1, image_size
            )
            
            # è®¡ç®—ç•¸å˜çŸ«æ­£æ˜ å°„
            map1, map2 = cv2.initUndistortRectifyMap(
                self.camera_matrix[robot_name][camera_type],
                self.dist_coeffs[robot_name][camera_type],
                None, new_camera_matrix, image_size, cv2.CV_16SC2
            )
            
            self.undistort_maps[robot_name][camera_type] = (map1, map2, roi)
            
            self.has_camera_info[robot_name][camera_type] = True
            rospy.loginfo(f"âœ… å·²è·å– {robot_name} {camera_type}ç›¸æœºçš„ç›¸æœºå‚æ•°å’Œç•¸å˜æ˜ å°„")
    
    def undistort_image(self, cv_image, robot_name, camera_type):
        """å¯¹å›¾åƒè¿›è¡Œå®æ—¶ç•¸å˜çŸ«æ­£"""
        if (not self.has_camera_info[robot_name][camera_type] or 
            self.undistort_maps[robot_name][camera_type] is None):
            return cv_image
        
        try:
            map1, map2, roi = self.undistort_maps[robot_name][camera_type]
            undistorted = cv2.remap(cv_image, map1, map2, cv2.INTER_LINEAR)
            
            # è£å‰ªROIåŒºåŸŸ
            x, y, w, h = roi
            if w > 0 and h > 0:
                undistorted = undistorted[y:y+h, x:x+w]
            
            return undistorted
        except Exception as e:
            rospy.logwarn(f"ç•¸å˜çŸ«æ­£å¤±è´¥: {e}")
            return cv_image
    
    def perspective_correction(self, cv_image):
        """å¯¹å›¾åƒè¿›è¡Œé€è§†å˜æ¢çŸ«æ­£"""
        try:
            corrected = self.perspective_corrector.auto_perspective_correction(cv_image)
            return corrected
        except Exception as e:
            rospy.logwarn(f"é€è§†å˜æ¢çŸ«æ­£å¤±è´¥: {e}")
            return cv_image
    
    def transform_coordinate_system(self, rvec, tvec):
        """
        å°†åæ ‡ä»OpenCVåæ ‡ç³»ï¼ˆXå³ï¼ŒYä¸‹ï¼ŒZå‰ï¼‰è½¬æ¢åˆ°ç›¸æœºåæ ‡ç³»ï¼ˆXå‰ï¼ŒZä¸Šï¼‰
        OpenCVåæ ‡ç³»ï¼šXå‘å³ï¼ŒYå‘ä¸‹ï¼ŒZå‘å‰
        ç›®æ ‡åæ ‡ç³»ï¼šXå‘å‰ï¼ŒZå‘ä¸Šï¼ŒYå‘å·¦ï¼ˆå³æ‰‹åæ ‡ç³»ï¼‰
        """
        # åˆ›å»ºä»OpenCVåˆ°ç›®æ ‡åæ ‡ç³»çš„æ—‹è½¬çŸ©é˜µ
        # è¿™ä¸ªæ—‹è½¬çŸ©é˜µå°†Xè½´ä»å³è½¬å‘å‰ï¼ŒZè½´ä»å‰è½¬å‘ä¸Š
        R_cv_to_cam = np.array([
            [0, 0, 1],   # X_cam = Z_cv (å‰)
            [-1, 0, 0],  # Y_cam = -X_cv (å·¦)  
            [0, -1, 0]   # Z_cam = -Y_cv (ä¸Š)
        ], dtype=np.float64)
        
        # å°†æ—‹è½¬å‘é‡è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
        R_obj, _ = cv2.Rodrigues(rvec)
        
        # åº”ç”¨åæ ‡ç³»å˜æ¢
        R_obj_cam = R_cv_to_cam @ R_obj
        tvec_cam = R_cv_to_cam @ tvec.reshape(3, 1)
        
        # å°†æ—‹è½¬çŸ©é˜µè½¬æ¢å›æ—‹è½¬å‘é‡
        rvec_cam, _ = cv2.Rodrigues(R_obj_cam)
        
        return rvec_cam, tvec_cam.flatten()
    
    def detect_aruco_markers_with_pose(self, cv_image, robot_name, camera_type):
        """
        æ£€æµ‹ARç å¹¶ä¼°è®¡ä½å§¿ï¼Œåœ¨å›¾åƒä¸Šç»˜åˆ¶3Dè½´å’Œè¾¹ç•Œæ¡†
        è¿”å›å¸¦æ ‡è®°çš„å›¾åƒã€æ£€æµ‹åˆ°çš„IDåˆ—è¡¨å’Œä½å§¿ä¿¡æ¯
        """
        if not self.has_camera_info[robot_name][camera_type]:
            # å¦‚æœæ²¡æœ‰ç›¸æœºå‚æ•°ï¼Œä½¿ç”¨åŸºæœ¬æ£€æµ‹
            return self.basic_detection(cv_image, robot_name, camera_type)
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        
        # æ£€æµ‹ARç 
        corners, ids, rejected = aruco.detectMarkers(gray, self.aruco_dict, parameters=self.parameters)
        
        detected_ids = []
        poses = []
        
        if ids is not None:
            # ä¼°è®¡ARç ä½å§¿
            rvecs, tvecs, _ = aruco.estimatePoseSingleMarkers(
                corners, self.marker_length, 
                self.camera_matrix[robot_name][camera_type], 
                self.dist_coeffs[robot_name][camera_type]
            )
            
            # å°†IDè½¬æ¢ä¸ºåˆ—è¡¨
            detected_ids = ids.flatten().tolist()
            
            # ç»˜åˆ¶æ£€æµ‹ç»“æœ
            for i, corner in enumerate(corners):
                marker_id = ids[i][0]
                rvec = rvecs[i]
                tvec = tvecs[i]
                
                # åæ ‡ç³»ç»Ÿè½¬æ¢ - å°†OpenCVåæ ‡è½¬æ¢ä¸ºç›¸æœºåæ ‡
                rvec_cam, tvec_cam = self.transform_coordinate_system(rvec, tvec)
                
                # å­˜å‚¨ä½å§¿ä¿¡æ¯ï¼ˆä½¿ç”¨è½¬æ¢åçš„åæ ‡ï¼‰
                poses.append({
                    'id': marker_id,
                    'rvec': rvec_cam,
                    'tvec': tvec_cam,
                    'camera_type': camera_type
                })
                
                # æ ¹æ®ç›¸æœºç±»å‹é€‰æ‹©é¢œè‰²
                if camera_type == 'front':
                    color = (0, 255, 0)  # ç»¿è‰² - å‰æ–¹ç›¸æœº
                else:
                    color = (255, 0, 0)  # è“è‰² - å³ä¾§ç›¸æœº
                
                # ç»˜åˆ¶ARç è¾¹ç•Œæ¡†
                cv2.polylines(cv_image, [corner.astype(int)], True, color, 2)
                
                # ç»˜åˆ¶3Dåæ ‡è½´ï¼ˆä½¿ç”¨åŸå§‹åæ ‡è¿›è¡Œç»˜åˆ¶ï¼Œå› ä¸ºOpenCVçš„ç»˜åˆ¶å‡½æ•°æœŸæœ›OpenCVåæ ‡ç³»ï¼‰
                axis_length = self.marker_length * 0.5
                cv2.drawFrameAxes(cv_image, self.camera_matrix[robot_name][camera_type], 
                                self.dist_coeffs[robot_name][camera_type], rvec, tvec, axis_length)
                
                # è®¡ç®—ä¸­å¿ƒç‚¹ç”¨äºæ˜¾ç¤ºID
                center = corner[0].mean(axis=0).astype(int)
                
                # ç»˜åˆ¶IDæ ‡ç­¾å’Œè·ç¦»ä¿¡æ¯ï¼ˆä½¿ç”¨è½¬æ¢åçš„è·ç¦»ï¼‰
                distance = np.linalg.norm(tvec_cam)
                text = f"ID:{marker_id} {camera_type} Dist:{distance:.2f}m"
                text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                
                # æ ‡ç­¾èƒŒæ™¯
                cv2.rectangle(cv_image, 
                            (center[0] - text_size[0]//2 - 5, center[1] - text_size[1] - 5),
                            (center[0] + text_size[0]//2 + 5, center[1] + 5),
                            color, -1)
                
                # IDæ–‡æœ¬
                cv2.putText(cv_image, text, 
                          (center[0] - text_size[0]//2, center[1]), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
                
                # å‘å¸ƒTFå˜æ¢ï¼ˆä½¿ç”¨è½¬æ¢åçš„åæ ‡ï¼‰
                self.publish_tf_transform(rvec_cam, tvec_cam, marker_id, robot_name, camera_type)
        
        return cv_image, detected_ids, poses
    
    def basic_detection(self, cv_image, robot_name, camera_type):
        """åŸºç¡€æ£€æµ‹ï¼ˆå½“æ²¡æœ‰ç›¸æœºå‚æ•°æ—¶ä½¿ç”¨ï¼‰"""
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        corners, ids, rejected = aruco.detectMarkers(gray, self.aruco_dict, parameters=self.parameters)
        
        detected_ids = []
        
        if ids is not None:
            detected_ids = ids.flatten().tolist()
            
            # æ ¹æ®ç›¸æœºç±»å‹é€‰æ‹©é¢œè‰²
            if camera_type == 'front':
                color = (0, 255, 0)  # ç»¿è‰²
            else:
                color = (255, 0, 0)  # è“è‰²
            
            for i, corner in enumerate(corners):
                marker_id = ids[i][0]
                corner = corner.astype(int)
                
                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                cv2.polylines(cv_image, [corner], True, color, 3)
                
                center = corner[0].mean(axis=0).astype(int)
                text = f"ID:{marker_id} {camera_type}"
                text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                
                cv2.rectangle(cv_image, 
                            (center[0] - text_size[0]//2 - 5, center[1] - text_size[1] - 5),
                            (center[0] + text_size[0]//2 + 5, center[1] + 5),
                            color, -1)
                
                cv2.putText(cv_image, text, 
                          (center[0] - text_size[0]//2, center[1]), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return cv_image, detected_ids, []
    
    def publish_tf_transform(self, rvec, tvec, marker_id, robot_name, camera_type):
        """å‘å¸ƒARç çš„TFå˜æ¢ï¼ŒåŒºåˆ†ä¸åŒç›¸æœº"""
        try:
            # å°†æ—‹è½¬å‘é‡è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
            rotation_matrix, _ = cv2.Rodrigues(rvec)
            
            # æ„å»ºé½æ¬¡å˜æ¢çŸ©é˜µ
            transform_matrix = np.eye(4)
            transform_matrix[:3, :3] = rotation_matrix
            transform_matrix[:3, 3] = tvec.flatten()
            
            # è½¬æ¢åˆ°TFæ ¼å¼
            transform = tf.transformations.rotation_matrix(0, (0, 0, 0))  # åˆ›å»ºå•ä½çŸ©é˜µ
            transform[:3, :3] = rotation_matrix
            transform[:3, 3] = tvec.flatten()
            
            # æå–å››å…ƒæ•°
            quaternion = tf.transformations.quaternion_from_matrix(transform)
            
            # æ ¹æ®ç›¸æœºç±»å‹ç¡®å®šçˆ¶åæ ‡ç³»
            if camera_type == 'front':
                parent_frame = f"{robot_name}/camera_link"
            else:
                parent_frame = f"{robot_name}/right_camera_link"
            
            # å‘å¸ƒTFï¼Œæ·»åŠ ç›¸æœºç±»å‹å‰ç¼€
            self.tf_broadcaster.sendTransform(
                tvec.flatten().tolist(),
                quaternion.tolist(),
                rospy.Time.now(),
                f"{camera_type}_aruco_{marker_id}",
                parent_frame
            )
            
        except Exception as e:
            rospy.logwarn(f"å‘å¸ƒTFå˜æ¢æ—¶å‡ºé”™: {e}")
    
    def publish_aruco_poses(self, poses, robot_name, camera_type):
        """å‘å¸ƒARç çš„åæ ‡ä¿¡æ¯"""
        if not poses:
            return
            
        pose_array = PoseArray()
        pose_array.header = Header()
        pose_array.header.stamp = rospy.Time.now()
        
        # è®¾ç½®åæ ‡ç³»
        if camera_type == 'front':
            pose_array.header.frame_id = f"{robot_name}/camera_link"
        else:
            pose_array.header.frame_id = f"{robot_name}/right_camera_link"
        
        for pose_info in poses:
            pose = Pose()
            
            # è®¾ç½®ä½ç½®
            tvec = pose_info['tvec'].flatten()
            pose.position = Point(x=tvec[0], y=tvec[1], z=tvec[2])
            
            # è®¾ç½®æ–¹å‘ï¼ˆå°†æ—‹è½¬å‘é‡è½¬æ¢ä¸ºå››å…ƒæ•°ï¼‰
            rvec = pose_info['rvec']
            rotation_matrix, _ = cv2.Rodrigues(rvec)
            
            # å°†æ—‹è½¬çŸ©é˜µè½¬æ¢ä¸ºå››å…ƒæ•°
            transform_matrix = np.eye(4)
            transform_matrix[:3, :3] = rotation_matrix
            quaternion = tf.transformations.quaternion_from_matrix(transform_matrix)
            
            pose.orientation = Quaternion(
                x=quaternion[0],
                y=quaternion[1],
                z=quaternion[2],
                w=quaternion[3]
            )
            
            pose_array.poses.append(pose)
        
        # å‘å¸ƒARç åæ ‡ä¿¡æ¯
        self.aruco_pose_publishers[robot_name][camera_type].publish(pose_array)
        
        # æ‰“å°åæ ‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
        if poses:
            for pose_info in poses:
                tvec = pose_info['tvec'].flatten()
                rospy.loginfo(f"ğŸ“Š {robot_name.upper()} {camera_type.upper()}ç›¸æœº ARç  {pose_info['id']} åæ ‡: "
                            f"X={tvec[0]:.3f}m, Y={tvec[1]:.3f}m, Z={tvec[2]:.3f}m")
    
    def image_callback(self, msg, robot_name, camera_type):
        """å¤„ç†å›¾åƒå›è°ƒï¼Œå…ˆè¿›è¡Œç•¸å˜çŸ«æ­£å†è¯†åˆ«ARç """
        try:
            # è½¬æ¢ROSå›¾åƒåˆ°OpenCVæ ¼å¼
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            
            # å®æ—¶ç•¸å˜çŸ«æ­£ - åœ¨è¯†åˆ«å‰å…ˆçŸ«æ­£å›¾åƒ
            undistorted_image = self.undistort_image(cv_image, robot_name, camera_type)
            
            # é€è§†å˜æ¢çŸ«æ­£ - è¿›ä¸€æ­¥çŸ«æ­£å›¾åƒå€¾æ–œ
            perspective_corrected_image = self.perspective_correction(undistorted_image)
            
            # æ£€æµ‹ARç ï¼ˆå¸¦ä½å§¿ä¼°è®¡ï¼‰
            result_image, detected_ids, poses = self.detect_aruco_markers_with_pose(perspective_corrected_image, robot_name, camera_type)
            
            # æ–°å¢ï¼šå‘å¸ƒARç åæ ‡ä¿¡æ¯
            self.publish_aruco_poses(poses, robot_name, camera_type)
            
            # åœ¨å›¾åƒå·¦ä¸Šè§’æ·»åŠ ç›¸æœºç±»å‹æ ‡è¯†
            cv2.putText(result_image, f"{camera_type.upper()} CAMERA - {robot_name}", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, 
                       (255, 255, 255) if camera_type == 'front' else (255, 200, 100), 2)
            
            # æ‰“å°æ£€æµ‹ç»“æœ
            if detected_ids:
                pose_info = ""
                for pose in poses:
                    tvec = pose['tvec'].flatten()
                    pose_info += f" ID{pose['id']}: pos({tvec[0]:.2f}, {tvec[1]:.2f}, {tvec[2]:.2f})"
                
                rospy.loginfo(f"ğŸ¤– {robot_name.upper()} {camera_type.upper()}ç›¸æœº æ£€æµ‹åˆ°ARç : IDs={detected_ids}{pose_info}")
            else:
                # å‡å°‘æœªæ£€æµ‹åˆ°æ—¶çš„æ—¥å¿—è¾“å‡ºé¢‘ç‡
                current_time = rospy.get_time()
                time_key = f'last_no_detection_time_{robot_name}_{camera_type}'
                
                if not hasattr(self, time_key):
                    setattr(self, time_key, 0)
                
                if current_time - getattr(self, time_key) > 5.0:
                    if self.has_camera_info[robot_name][camera_type]:
                        rospy.loginfo(f"ğŸ” {robot_name.upper()} {camera_type.upper()}ç›¸æœº è§†é‡å†…æœªæ£€æµ‹åˆ°ARç ")
                    else:
                        rospy.logwarn(f"âš ï¸ {robot_name.upper()} {camera_type.upper()}ç›¸æœº ç­‰å¾…ç›¸æœºå‚æ•°...")
                    setattr(self, time_key, current_time)
            
            # å‘å¸ƒç»“æœå›¾åƒ
            try:
                result_msg = self.bridge.cv2_to_imgmsg(result_image, "bgr8")
                self.image_publishers[robot_name][camera_type].publish(result_msg)
            except CvBridgeError as e:
                rospy.logerr(f"å‘å¸ƒå›¾åƒæ—¶å‡ºé”™ ({robot_name} {camera_type}): {e}")
                
        except CvBridgeError as e:
            rospy.logerr(f"å›¾åƒè½¬æ¢é”™è¯¯ ({robot_name} {camera_type}): {e}")
        except Exception as e:
            rospy.logerr(f"å¤„ç†å›¾åƒæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ ({robot_name} {camera_type}): {e}")
    
    def run(self):
        """è¿è¡ŒèŠ‚ç‚¹"""
        rospy.loginfo("å¢å¼ºç‰ˆARç æ£€æµ‹å™¨è¿è¡Œä¸­...æŒ‰Ctrl+Cé€€å‡º")
        rospy.loginfo("æ”¯æŒå®æ—¶ç•¸å˜çŸ«æ­£ã€é€è§†å˜æ¢çŸ«æ­£å’ŒåŒç›¸æœºARç è¯†åˆ«")
        rospy.loginfo("ARç åæ ‡è¯é¢˜æ ¼å¼: /aruco_detection/<robot_name>/<camera_type>_poses")
        rospy.loginfo("åæ ‡ç³»å·²è½¬æ¢ä¸º: Xè½´æœå‰ï¼ŒZè½´æœä¸Š")
        rospy.spin()

if __name__ == '__main__':
    try:
        detector = EnhancedArucoDetector()
        detector.run()
    except rospy.ROSInterruptException:
        rospy.loginfo("å¢å¼ºç‰ˆARç æ£€æµ‹å™¨å·²å…³é—­")